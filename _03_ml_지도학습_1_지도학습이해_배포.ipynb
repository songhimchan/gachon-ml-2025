{"cells":[{"cell_type":"markdown","source":["# ì§€ë„í•™ìŠµ ì´í•´\n","\n","\n","---\n","---\n","\n"],"metadata":{"id":"PCVZEUPUNldz"},"id":"PCVZEUPUNldz"},{"cell_type":"markdown","source":["## **1. ì§€ë„í•™ìŠµ(supervised learning) ì´í•´**\n","- ì •ë‹µ(label)ì´ ì£¼ì–´ì§„ ë°ì´í„°ë¥¼ ì´ìš©í•´ ëª¨ë¸ì„ í•™ìŠµì‹œí‚¤ëŠ” ê¸°ê³„í•™ìŠµ ë°©ì‹\n","- ëª¨ë¸ì€ ì…ë ¥(X)ê³¼ ì¶œë ¥(Y) ê°„ì˜ ê´€ê³„ë¥¼ í•™ìŠµí•˜ê³ , ìƒˆë¡œìš´ ì…ë ¥ì— ëŒ€í•œ ì˜ˆì¸¡ì„ ìˆ˜í–‰"],"metadata":{"id":"mcvw1xqqXRQP"},"id":"mcvw1xqqXRQP"},{"cell_type":"markdown","source":["### **[ì‹¤ìŠµ] ìì—°ì–´ ì²˜ë¦¬(NLP) ë¶„ì•¼ì—ì„œì˜ ì§€ë„í•™ìŠµ(ë¶„ë¥˜) ì˜ˆ**\n"],"metadata":{"id":"cJ3tF5MFFB2o"},"id":"cJ3tF5MFFB2o"},{"cell_type":"markdown","metadata":{"id":"QjvcIpHmGuFF"},"source":["#### **ì˜ˆì œ1. í…ìŠ¤íŠ¸ ë¶„ë¥˜(ë‰´ìŠ¤ ì£¼ì œ ë¶„ë¥˜)**\n","- **íƒœìŠ¤í¬** : ë‰´ìŠ¤ ì£¼ì œ ë¶„ë¥˜\n","- **ë¬¸ì œìœ í˜•** :  \n","- **ë°ì´í„°ì…‹** :  \n","- **íŠ¹ì§•ì¶”ì¶œ** : TF-IDF(N-gram: bigram)\n","- **ë¶„ë¥˜ê¸°** :  \n","- **í•µì‹¬ë‚´ìš©**: ê°€ì¥ ê¸°ë³¸ì ì¸ ë¬¸ì„œ ë¶„ë¥˜ íŒŒì´í”„ë¼ì¸"],"id":"QjvcIpHmGuFF"},{"cell_type":"code","execution_count":null,"metadata":{"id":"AhUYjyga-mAw"},"outputs":[],"source":["# --- 1. í…ìŠ¤íŠ¸ ë¶„ë¥˜: 20 Newsgroups ---\n","from sklearn.datasets import fetch_20newsgroups\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.pipeline import make_pipeline\n","from sklearn.metrics import classification_report, accuracy_score\n","\n","\n","# 1) ë°ì´í„° ë¡œë“œ (í›ˆë ¨/í…ŒìŠ¤íŠ¸ ë¶„ë¦¬ ì œê³µ)\n","cats = ['sci.space', 'rec.sport.baseball', 'comp.graphics', 'talk.politics.mideast']\n","train = fetch_20newsgroups(subset='train', categories=cats, remove=('headers','footers','quotes'))\n","test  = fetch_20newsgroups(subset='test',  categories=cats, remove=('headers','footers','quotes'))\n","\n","\n","# 2) íŒŒì´í”„ë¼ì¸: TF-IDF â†’ ë¡œì§€ìŠ¤í‹± íšŒê·€(ë©€í‹°í´ë˜ìŠ¤)\n","pipe = make_pipeline(TfidfVectorizer(min_df=3, ngram_range=(1,2)),\n","                     LogisticRegression(max_iter=300))\n","\n","\n","# 3) í•™ìŠµ\n","pipe.fit(train.data, train.target)\n","\n","\n","# 4) í‰ê°€\n","pred = pipe.predict(test.data)\n","print(\"Accuracy:\", accuracy_score(test.target, pred))\n","print(classification_report(test.target, pred, target_names=test.target_names))\n","\n","\n","# 5) ì‚¬ìš© ì˜ˆì‹œ\n","sample = [\"SpaceX just launched another rocket to the ISS.\",\n","          \"OpenGL shaders can speed up graphics rendering.\",\n","          \"The team won the baseball world series!\",\n","          \"The negotiation in the middle east escalated.\"]\n","print(pipe.predict(sample))          # ì˜ˆì¸¡ëœ ì •ìˆ˜ ë¼ë²¨\n","print([test.target_names[i] for i in pipe.predict(sample)])  # ë¼ë²¨ ì´ë¦„\n"],"id":"AhUYjyga-mAw"},{"cell_type":"markdown","source":["#### **ì˜ˆì œ2. ê°ì„± ë¶„ë¥˜(ê¸/ë¶€ì •)**\n","NLTKì˜ movie_reviews ì½”í¼ìŠ¤(ê¸/ë¶€ì • ë¼ë²¨)ë¥¼ ì‚¬ìš©í•´ ë¬¸ì„œ ë¶„ë¥˜(ì´ì§„ ë¶„ë¥˜)ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.\n","\n","- **íƒœìŠ¤í¬** : ê°ì„ ë¶„ì„\n","- **ë¬¸ì œìœ í˜•** :  \n","- **ë°ì´í„°ì…‹** :  \n","- **íŠ¹ì§•ì¶”ì¶œ** : TF-IDF(N-gram: bigram)\n","- **ë¶„ë¥˜ê¸°** :  \n","- **í•µì‹¬ë‚´ìš©**: ë¦¬ë·° í…ìŠ¤íŠ¸ì˜ ê¸/ë¶€ì • íŒ"],"metadata":{"id":"_brMjLktJrao"},"id":"_brMjLktJrao"},{"cell_type":"code","source":["# --- 2. ê°ì„± ë¶„ì„: NLTK movie_reviews (ê¸/ë¶€ì •) ---\n","import nltk\n","nltk.download('movie_reviews')\n","\n","from nltk.corpus import movie_reviews\n","from sklearn.model_selection import train_test_split\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.svm import LinearSVC\n","from sklearn.pipeline import make_pipeline\n","from sklearn.metrics import classification_report, accuracy_score\n","\n","# 1) ë¬¸ì„œ/ë¼ë²¨ êµ¬ì„±\n","docs = []\n","labels = []\n","for cat in movie_reviews.categories():          # 'pos', 'neg'\n","    for fid in movie_reviews.fileids(cat):\n","        docs.append(movie_reviews.raw(fid))\n","        labels.append(cat)\n","\n","# 2) í•™ìŠµ/ê²€ì¦ ë¶„ë¦¬\n","X_train, X_test, y_train, y_test = train_test_split(\n","    docs, labels, test_size=0.2, random_state=42, stratify=labels\n",")\n","\n","# 3) íŒŒì´í”„ë¼ì¸: TF-IDF â†’ Linear SVM\n","pipe = make_pipeline(TfidfVectorizer(min_df=3, ngram_range=(1,2)),\n","                     LinearSVC())\n","\n","# 4) í•™ìŠµ & í‰ê°€\n","pipe.fit(X_train, y_train)\n","pred = pipe.predict(X_test)\n","print(\"Accuracy:\", accuracy_score(y_test, pred))\n","print(classification_report(y_test, pred))\n","\n","# 5) ì‚¬ìš© ì˜ˆì‹œ\n","samples = [\n","    \"What a wonderful movie! Brilliant acting and a touching story.\",\n","    \"It was boring and way too long. I wouldn't recommend it.\"\n","]\n","print(pipe.predict(samples))  # ['pos', 'neg'] ì˜ˆìƒ\n"],"metadata":{"id":"YPfSobv_cllS"},"id":"YPfSobv_cllS","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8LOWA7jQGuFH"},"source":["#### **ì˜ˆì œ3. í’ˆì‚¬ íƒœê¹…(POS Tagging)**\n","í† í° ë‹¨ìœ„ë¡œ í’ˆì‚¬ ë¼ë²¨(POS tag) ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ì‹œí€€ìŠ¤ ë¼ë²¨ë§ì„, ê°„ë‹¨í•œ ë§¥ë½ ê¸°ë°˜ íŠ¹ì§• + ë¡œì§€ìŠ¤í‹± íšŒê·€ë¡œ êµ¬í˜„\n","- **íƒœìŠ¤í¬** : í’ˆì‚¬ íƒœê¹…\n","- **ë¬¸ì œìœ í˜•** : ì‹œí€€ìŠ¤ ë¼ë²¨ë§(í† í° ë‹¨ìœ„ ë¶„ë¥˜)\n","- **ë°ì´í„°ì…‹** :  \n","- **íŠ¹ì§•ì¶”ì¶œ** : ì£¼ë³€ ë‹¨ì–´/ì ë¯¸ì‚¬ ë“± ìˆ˜ì‘ì—… íŠ¹ì§•\n","- **ë¶„ë¥˜ê¸°** :  \n","- **í•µì‹¬ë‚´ìš©**: ì§€ë„í•™ìŠµì˜ í•µì‹¬ì¸ íŠ¹ì§•-->ë¼ë²¨) ì²´"],"id":"8LOWA7jQGuFH"},{"cell_type":"code","execution_count":null,"metadata":{"id":"Hj_L_F9RH5B0"},"outputs":[],"source":["# --- 3. í’ˆì‚¬ íƒœê¹…: NLTK treebank + ê°„ë‹¨ íŠ¹ì§• + ë¡œì§€ìŠ¤í‹± íšŒê·€ ---\n","import nltk\n","nltk.download('treebank')\n","nltk.download('universal_tagset')\n","\n","from nltk.corpus import treebank\n","from sklearn.feature_extraction import DictVectorizer\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import classification_report, accuracy_score\n","from sklearn.model_selection import train_test_split\n","\n","# 1) (ë‹¨ì–´, íƒœê·¸) ì‹œí€€ìŠ¤ ë¶ˆëŸ¬ì˜¤ê¸° (ë³´í¸ íƒœê·¸ì…‹ ì‚¬ìš©: NOUN, VERB, ADJ ë“±)\n","sents = treebank.tagged_sents(tagset='universal')\n","\n","# ê°„ë‹¨í•œ íŠ¹ì§• í•¨ìˆ˜: í˜„ì¬/ì´ì „/ë‹¤ìŒ ë‹¨ì–´, ì ‘ë¯¸ì‚¬/ëŒ€ë¬¸ì/ìˆ«ì ë“±\n","def token_features(sent_words, i):\n","    w = sent_words[i]\n","    prev_w = sent_words[i-1] if i-1 >= 0 else \"<BOS>\"\n","    next_w = sent_words[i+1] if i+1 < len(sent_words) else \"<EOS>\"\n","    feats = {\n","        \"w.lower\": w.lower(),\n","        \"w.isupper\": w.isupper(),\n","        \"w.isdigit\": w.isdigit(),\n","        \"w.suffix2\": w[-2:].lower(),\n","        \"w.suffix3\": w[-3:].lower(),\n","        \"prev.lower\": prev_w.lower(),\n","        \"next.lower\": next_w.lower(),\n","        \"prev.isupper\": prev_w.isupper() if isinstance(prev_w, str) else False,\n","        \"next.isupper\": next_w.isupper() if isinstance(next_w, str) else False,\n","    }\n","    return feats\n","\n","# 2) íŠ¹ì§•/ë¼ë²¨ ë²¡í„° ë§Œë“¤ê¸°\n","X_dict, y = [], []\n","for sent in sents:\n","    words = [w for w, t in sent]\n","    tags  = [t for w, t in sent]\n","    for i in range(len(words)):\n","        X_dict.append(token_features(words, i))\n","        y.append(tags[i])\n","\n","# 3) í›ˆë ¨/í…ŒìŠ¤íŠ¸ ë¶„ë¦¬\n","X_train_dict, X_test_dict, y_train, y_test = train_test_split(\n","    X_dict, y, test_size=0.2, random_state=42, stratify=y\n",")\n","\n","# 4) Dict â†’ ë²¡í„°í™” â†’ ë¡œì§€ìŠ¤í‹± íšŒê·€\n","vec = DictVectorizer(sparse=True)\n","X_train = vec.fit_transform(X_train_dict)\n","X_test  = vec.transform(X_test_dict)\n","\n","clf = LogisticRegression(max_iter=300, n_jobs=None, multi_class='auto')\n","clf.fit(X_train, y_train)\n","\n","# 5) í‰ê°€\n","pred = clf.predict(X_test)\n","print(\"Token-level Accuracy:\", accuracy_score(y_test, pred))\n","print(classification_report(y_test, pred))\n"],"id":"Hj_L_F9RH5B0"},{"cell_type":"markdown","source":["### **[ì‹¤ìŠµ] ìì—°ì–´ ì²˜ë¦¬(NLP) ë¶„ì•¼ì—ì„œì˜ ì§€ë„í•™ìŠµ(íšŒê·€) ì˜ˆ**"],"metadata":{"id":"Op84MOfWS-pO"},"id":"Op84MOfWS-pO"},{"cell_type":"markdown","source":["#### **ì˜ˆì œ1 : ì˜í™” ë¦¬ë·° ê°ì • ì ìˆ˜ ì˜ˆì¸¡ (1-5ì  ì²™ë„)**"],"metadata":{"id":"_Ewf8k1dTBco"},"id":"_Ewf8k1dTBco"},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.linear_model import LinearRegression\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import mean_squared_error, r2_score\n","import re\n","from nltk.corpus import stopwords\n","import nltk\n","\n","# ìƒ˜í”Œ ë°ì´í„° ìƒì„±\n","reviews = [\n","    \"This movie was absolutely amazing! Perfect story and acting.\",\n","    \"Great film with excellent cinematography and soundtrack.\",\n","    \"It was okay, nothing special but watchable.\",\n","    \"Not my favorite, but had some good moments.\",\n","    \"Terrible movie, waste of time and money.\",\n","    \"Outstanding performance by all actors!\",\n","    \"Average movie with predictable plot.\"\n","]\n","\n","ratings = [5.0, 4.5, 3.0, 2.5, 1.0, 4.8, 3.2]\n","\n","# í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ í•¨ìˆ˜\n","def preprocess_text(text):\n","    # ì†Œë¬¸ì ë³€í™˜\n","    text = text.lower()\n","    # íŠ¹ìˆ˜ë¬¸ì ì œê±°\n","    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n","    return text\n","\n","# ì „ì²˜ë¦¬ ì ìš©\n","processed_reviews = [preprocess_text(review) for review in reviews]\n","\n","# TF-IDF ë²¡í„°í™”\n","vectorizer = TfidfVectorizer(max_features=100, stop_words='english')\n","X = vectorizer.fit_transform(processed_reviews)\n","\n","# ë°ì´í„° ë¶„í• \n","X_train, X_test, y_train, y_test = train_test_split(\n","    X, ratings, test_size=0.3, random_state=42\n",")\n","\n","# ì„ í˜• íšŒê·€ ëª¨ë¸ í›ˆë ¨\n","model = LinearRegression()\n","model.fit(X_train, y_train)\n","\n","# ì˜ˆì¸¡ ë° í‰ê°€\n","y_pred = model.predict(X_test)\n","mse = mean_squared_error(y_test, y_pred)\n","r2 = r2_score(y_test, y_pred)\n","\n","print(f\"MSE: {mse:.3f}\")\n","print(f\"RÂ² Score: {r2:.3f}\")\n","print(f\"ì‹¤ì œê°’: {y_test}\")\n","print(f\"ì˜ˆì¸¡ê°’: {y_pred}\")\n","\n","print()\n","ìƒëŒ€ì˜¤ì°¨ìœ¨ = ((y_test - y_pred) / y_test) * 100\n","print(f\"ìƒëŒ€ì˜¤ì°¨ìœ¨: {ìƒëŒ€ì˜¤ì°¨ìœ¨}%\")"],"metadata":{"id":"agvUUeXwTCQx"},"id":"agvUUeXwTCQx","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### **ì˜ˆì œ2 : í…ìŠ¤íŠ¸ ê°€ë…ì„± ì ìˆ˜ ì˜ˆì¸¡**"],"metadata":{"id":"uFm6BXesTBqU"},"id":"uFm6BXesTBqU"},{"cell_type":"code","source":["!pip install textstat"],"metadata":{"id":"QhjFxsvITjIh"},"id":"QhjFxsvITjIh","execution_count":null,"outputs":[]},{"cell_type":"code","source":["import textstat\n","from sklearn.ensemble import RandomForestRegressor\n","from sklearn.preprocessing import StandardScaler\n","\n","# ìƒ˜í”Œ í…ìŠ¤íŠ¸ì™€ ê°€ë…ì„± ì ìˆ˜ (Flesch Reading Ease ê¸°ì¤€)\n","texts = [\n","    \"The cat sat on the mat.\",\n","    \"Artificial intelligence represents a paradigm shift in computational methodologies.\",\n","    \"Machine learning algorithms can process large amounts of data efficiently.\",\n","    \"Deep learning uses neural networks with multiple layers.\",\n","    \"This is a simple sentence that anyone can understand easily.\"\n","]\n","\n","# ì‹¤ì œ Flesch Reading Ease ì ìˆ˜ ê³„ì‚°\n","readability_scores = [textstat.flesch_reading_ease(text) for text in texts]\n","\n","# íŠ¹ì§• ì¶”ì¶œ (ë¬¸ì¥ ê¸¸ì´, ë‹¨ì–´ ìˆ˜, ìŒì ˆ ìˆ˜ ë“±)\n","def extract_features(text):\n","    words = text.split()\n","    sentences = text.split('.')\n","\n","    return [\n","        len(words),  # ë‹¨ì–´ ìˆ˜\n","        len(sentences),  # ë¬¸ì¥ ìˆ˜\n","        sum(len(word) for word in words) / len(words),  # í‰ê·  ë‹¨ì–´ ê¸¸ì´\n","        textstat.syllable_count(text),  # ìŒì ˆ ìˆ˜\n","        len(text)  # ì´ ê¸€ì ìˆ˜\n","    ]\n","\n","# íŠ¹ì§• ë²¡í„° ìƒì„±\n","features = [extract_features(text) for text in texts]\n","X = np.array(features)\n","y = np.array(readability_scores)\n","\n","# ì •ê·œí™”\n","scaler = StandardScaler()\n","X_scaled = scaler.fit_transform(X)\n","\n","# Random Forest íšŒê·€ ëª¨ë¸\n","rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n","rf_model.fit(X_scaled, y)\n","\n","# ìƒˆë¡œìš´ í…ìŠ¤íŠ¸ ì˜ˆì¸¡\n","new_text = \"Python is a programming language that is easy to learn and use.\"\n","new_features = scaler.transform([extract_features(new_text)])\n","predicted_score = rf_model.predict(new_features)[0]\n","\n","print(f\"ìƒˆë¡œìš´ í…ìŠ¤íŠ¸: {new_text}\")\n","print(f\"ì˜ˆì¸¡ëœ ê°€ë…ì„± ì ìˆ˜: {predicted_score:.2f}\")\n","print(f\"ì‹¤ì œ ê°€ë…ì„± ì ìˆ˜: {textstat.flesch_reading_ease(new_text):.2f}\")\n","\n","print()\n","actual = textstat.flesch_reading_ease(new_text)\n","ìƒëŒ€ì˜¤ì°¨ìœ¨ = ((actual - predicted_score) / actual) * 100\n","print(f\"ìƒëŒ€ì˜¤ì°¨ìœ¨: {ìƒëŒ€ì˜¤ì°¨ìœ¨:.2f}%\")"],"metadata":{"id":"vxgZah7VTCnJ"},"id":"vxgZah7VTCnJ","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### **ì˜ˆì œ3 : ë¬¸ì„œ ìœ ì‚¬ë„ ì ìˆ˜ ì˜ˆì¸¡**"],"metadata":{"id":"_HGmR2QETB0E"},"id":"_HGmR2QETB0E"},{"cell_type":"code","source":["from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.metrics.pairwise import cosine_similarity\n","from sklearn.svm import SVR\n","import numpy as np\n","\n","# ìƒ˜í”Œ ë¬¸ì„œ ìŒê³¼ ìœ ì‚¬ë„ ì ìˆ˜\n","doc_pairs = [\n","    (\"The weather is nice today\", \"Today has beautiful weather\"),\n","    (\"I love programming\", \"Programming is my passion\"),\n","    (\"The cat is sleeping\", \"The dog is running\"),\n","    (\"Machine learning is fascinating\", \"AI and ML are interesting topics\"),\n","    (\"Python is great for data science\", \"Data analysis with Python is powerful\")\n","]\n","\n","# ì‹¤ì œ ìœ ì‚¬ë„ ì ìˆ˜ (ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê¸°ë°˜)\n","similarity_scores = []\n","for doc1, doc2 in doc_pairs:\n","    vectorizer = CountVectorizer().fit([doc1, doc2])\n","    vectors = vectorizer.transform([doc1, doc2])\n","    similarity = cosine_similarity(vectors[0], vectors[1])[0][0]\n","    similarity_scores.append(similarity)\n","\n","# íŠ¹ì§• ì¶”ì¶œ (ë‘ ë¬¸ì„œì˜ íŠ¹ì§• ê²°í•©)\n","def extract_pair_features(doc1, doc2):\n","    # ê¸°ë³¸ í†µê³„\n","    len_diff = abs(len(doc1.split()) - len(doc2.split()))\n","\n","    # ê³µí†µ ë‹¨ì–´ ìˆ˜\n","    words1 = set(doc1.lower().split())\n","    words2 = set(doc2.lower().split())\n","    common_words = len(words1.intersection(words2))\n","\n","    # Jaccard ìœ ì‚¬ë„\n","    jaccard_sim = len(words1.intersection(words2)) / len(words1.union(words2))\n","\n","    return [len_diff, common_words, jaccard_sim]\n","\n","# íŠ¹ì§• ë²¡í„° ìƒì„±\n","features = [extract_pair_features(doc1, doc2) for doc1, doc2 in doc_pairs]\n","X = np.array(features)\n","y = np.array(similarity_scores)\n","\n","# SVR ëª¨ë¸ í›ˆë ¨\n","svr_model = SVR(kernel='rbf', C=1.0, gamma='scale')\n","svr_model.fit(X, y)\n","\n","# ì˜ˆì¸¡ í…ŒìŠ¤íŠ¸\n","test_pair = (\"Deep learning networks\", \"Neural networks for AI\")\n","test_features = np.array([extract_pair_features(test_pair[0], test_pair[1])])\n","predicted_similarity = svr_model.predict(test_features)[0]\n","\n","print(f\"í…ŒìŠ¤íŠ¸ ë¬¸ì„œ ìŒ:\")\n","print(f\"ë¬¸ì„œ 1: {test_pair[0]}\")\n","print(f\"ë¬¸ì„œ 2: {test_pair[1]}\")\n","print(f\"ì˜ˆì¸¡ëœ ìœ ì‚¬ë„: {predicted_similarity:.3f}\")\n","\n","# ì‹¤ì œ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ì™€ ë¹„êµ\n","vectorizer = CountVectorizer().fit(test_pair)\n","vectors = vectorizer.transform(test_pair)\n","actual_similarity = cosine_similarity(vectors[0], vectors[1])[0][0]\n","print(f\"ì‹¤ì œ ì½”ì‚¬ì¸ ìœ ì‚¬ë„: {actual_similarity:.3f}\")\n","\n","print()\n","ìƒëŒ€ì˜¤ì°¨ìœ¨ = ((actual_similarity - predicted_similarity) / actual_similarity) * 100\n","print(f\"ìƒëŒ€ì˜¤ì°¨ìœ¨: {ìƒëŒ€ì˜¤ì°¨ìœ¨:.2f}%\")"],"metadata":{"id":"9DEcH4wlTC95"},"id":"9DEcH4wlTC95","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","\n","---\n","\n"],"metadata":{"id":"NfOPlbYULOSR"},"id":"NfOPlbYULOSR"},{"cell_type":"markdown","source":["## **2. AI ëª¨ë¸ë§ í”„ë¡œì„¸ìŠ¤**"],"metadata":{"id":"X_mujQclLNMV"},"id":"X_mujQclLNMV"},{"cell_type":"markdown","source":["\n","\n","---\n","\n"],"metadata":{"id":"IrX2xVfCLNQM"},"id":"IrX2xVfCLNQM"},{"cell_type":"markdown","source":["## **3. í•™ìŠµë°ì´í„° ë¶„í•  ë°©ë²•**"],"metadata":{"id":"69ZboNqcLnXv"},"id":"69ZboNqcLnXv"},{"cell_type":"markdown","source":["### ì˜ˆì œ : K-Fold êµì°¨ê²€ì¦ (íšŒê·€)"],"metadata":{"id":"GTkAgDPZZRsK"},"id":"GTkAgDPZZRsK"},{"cell_type":"code","source":["import numpy as np\n","from sklearn.model_selection import KFold, cross_val_score, train_test_split\n","from sklearn.datasets import make_regression\n","from sklearn.linear_model import LinearRegression\n","from sklearn.metrics import r2_score\n","\n","# ë°ì´í„° ìƒì„±\n","# X, y = make_regression(n_samples=1000, n_features=10, noise=10, random_state=42)\n","# X, y = make_regression(\n","#     n_samples=200,      # ì‘ì€ ë°ì´í„°ì…‹\n","#     n_features=15,      # íŠ¹ì§• ìˆ˜ ì¦ê°€\n","#     n_informative=8,    # ìœ ìš©í•œ íŠ¹ì§• ìˆ˜\n","#     noise=30,           # ë†’ì€ ë…¸ì´ì¦ˆ\n","#     random_state=42\n","# )\n","# # ë” í° ì°¨ì´ë¥¼ ë³´ì´ëŠ” ë°ì´í„°\n","X, y = make_regression(\n","    n_samples=100,      # ë§¤ìš° ì‘ì€ ë°ì´í„°ì…‹\n","    n_features=20,      # íŠ¹ì§• ìˆ˜ ë” ì¦ê°€\n","    n_informative=5,    # ìœ ìš©í•œ íŠ¹ì§•ì€ ì ê²Œ\n","    noise=50,           # ë§¤ìš° ë†’ì€ ë…¸ì´ì¦ˆ\n","    random_state=123    # ë‹¤ë¥¸ ì‹œë“œê°’\n",")\n","\n","def compare_evaluation_methods(X, y):\n","    \"\"\"\n","    ë‹¨ì¼ ë¶„í•  vs K-Fold êµì°¨ê²€ì¦ ê°„ë‹¨ ë¹„êµ\n","    \"\"\"\n","    print(\"=== êµì°¨ê²€ì¦ ìœ ë¬´ ë¹„êµ ===\")\n","\n","    # 1. ë‹¨ì¼ ë¶„í•  (êµì°¨ê²€ì¦ X)\n","    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","    model = LinearRegression()\n","    model.fit(X_train, y_train)\n","    y_pred = model.predict(X_test)\n","    single_r2 = r2_score(y_test, y_pred)\n","\n","    # 2. K-Fold êµì°¨ê²€ì¦ (êµì°¨ê²€ì¦ O)\n","    kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n","    cv_scores = cross_val_score(LinearRegression(), X, y, cv=kfold, scoring='r2')\n","    kfold_r2_mean = cv_scores.mean()\n","    kfold_r2_std = cv_scores.std()\n","\n","    # ê²°ê³¼ ì¶œë ¥\n","    print(f\"ë‹¨ì¼ ë¶„í•  RÂ²: {single_r2:.3f}\")\n","    print(f\"K-Fold    RÂ²: {kfold_r2_mean:.3f} Â± {kfold_r2_std:.3f}\")\n","    print(f\"ì°¨ì´: {abs(single_r2 - kfold_r2_mean):.3f}\")\n","\n","    if kfold_r2_std < 0.05:\n","        print(\"âœ… K-Fold ê²°ê³¼ê°€ ì•ˆì •ì ì…ë‹ˆë‹¤.\")\n","    else:\n","        print(\"âš ï¸ ì„±ëŠ¥ ë³€ë™ì´ ìˆìŠµë‹ˆë‹¤.\")\n","\n","# ì‹¤í–‰\n","compare_evaluation_methods(X, y)"],"metadata":{"id":"Jwxg5bxyZR1b"},"id":"Jwxg5bxyZR1b","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### [ì‹¤ìŠµ] í•™ìŠµ ë°ì´í„° ë¶„í•  ë°©ë²• í…ŒìŠ¤íŠ¸\n","- ì•ì˜ **2.ê°ì„± ë¶„ë¥˜ ê³¼ì •**ì—ì„œ í•™ìŠµ/í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¶„ë¥˜ë¥¼ 5:5, 9:1ë¡œ í–ˆì„ ë•Œ ëª¨ë¸ì˜ ì •í™•ë„ê°€ ë³€ë™ì´ ìˆëŠ”ì§€ í™•ì¸í•´ ë³´ì„¸ìš”.\n","- **8:1** = Accuracy: **0.87**\n","- **5:5** = Accuracy: **?**\n","- **9:1** = Accuracy: **?**"],"metadata":{"id":"FGyTgeBzMMBB"},"id":"FGyTgeBzMMBB"},{"cell_type":"code","source":["# --- 2. ê°ì„± ë¶„ì„: NLTK movie_reviews (ê¸/ë¶€ì •) ---\n","import nltk\n","nltk.download('movie_reviews')\n","\n","from nltk.corpus import movie_reviews\n","from sklearn.model_selection import train_test_split\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.svm import LinearSVC\n","from sklearn.pipeline import make_pipeline\n","from sklearn.metrics import classification_report, accuracy_score\n","\n","# 1) ë¬¸ì„œ/ë¼ë²¨ êµ¬ì„±\n","docs = []\n","labels = []\n","for cat in movie_reviews.categories():          # 'pos', 'neg'\n","    for fid in movie_reviews.fileids(cat):\n","        docs.append(movie_reviews.raw(fid))\n","        labels.append(cat)\n"],"metadata":{"id":"-pWdlhC_Mt-N"},"id":"-pWdlhC_Mt-N","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["- **ì¡°ê±´1: í•™ìŠµ:í…ŒìŠ¤íŠ¸=5:5**"],"metadata":{"id":"aM1pLCKfMuvD"},"id":"aM1pLCKfMuvD"},{"cell_type":"code","execution_count":null,"metadata":{"id":"z0ywO_s4S98S"},"outputs":[],"source":["\n","\n"],"id":"z0ywO_s4S98S"},{"cell_type":"markdown","source":["- **ì¡°ê±´2: í•™ìŠµ:í…ŒìŠ¤íŠ¸=9:1**"],"metadata":{"id":"aRWc89ukfTpj"},"id":"aRWc89ukfTpj"},{"cell_type":"code","execution_count":null,"metadata":{"id":"7W7R0Z27fTpk"},"outputs":[],"source":["\n","\n"],"id":"7W7R0Z27fTpk"},{"cell_type":"markdown","source":["\n","\n","---\n","\n"],"metadata":{"id":"MIKJjgsyLnhO"},"id":"MIKJjgsyLnhO"},{"cell_type":"markdown","metadata":{"id":"-RU-Qs13GuFI"},"source":["## **4.ëª¨ë¸ í‰ê°€ ë°©ë²•**"],"id":"-RU-Qs13GuFI"},{"cell_type":"markdown","source":["### ì˜ˆì œ : íšŒê·€ëª¨ë¸ í‰ê°€ì§€í‘œ"],"metadata":{"id":"uLXB79xlcV8g"},"id":"uLXB79xlcV8g"},{"cell_type":"markdown","source":["|ì§€í‘œ | ê³µì‹ | íŠ¹ì§•  | í•´ì„ |\n","|---|---|---|--|\n","|MSE  | í‰ê· ((ì‹¤ì œ-ì˜ˆì¸¡)Â²) | ì´ìƒì¹˜ì— ë¯¼ê° | ì‘ì„ìˆ˜ë¡ ì¢‹ìŒ |\n","|MAEí‰ê·  | (|ì‹¤ì œ-ì˜ˆì¸¡|) | ì´ìƒì¹˜ì— ê°•ê±´ | ì‘ì„ìˆ˜ë¡ ì¢‹ìŒ|\n","|RÂ² | 1 - (ì˜¤ì°¨ë¶„ì‚°/ì „ì²´ë¶„ì‚°) | ì„¤ëª…ë ¥ ì¸¡ì • | 1ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ì¢‹ìŒ |"],"metadata":{"id":"LwgeF8HUdFSY"},"id":"LwgeF8HUdFSY"},{"cell_type":"code","execution_count":null,"metadata":{"id":"djyDIbjmTDag"},"outputs":[],"source":["import numpy as np\n","from sklearn.datasets import make_regression\n","from sklearn.linear_model import LinearRegression\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n","\n","# ìƒ˜í”Œ ë°ì´í„° ìƒì„±\n","X, y = make_regression(n_samples=100, n_features=5, noise=10, random_state=42)\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# ëª¨ë¸ í›ˆë ¨ ë° ì˜ˆì¸¡\n","model = LinearRegression()\n","model.fit(X_train, y_train)\n","y_pred = model.predict(X_test)\n","\n","print(\"=== íšŒê·€ ëª¨ë¸ í‰ê°€ì§€í‘œ 3ê°€ì§€ ===\")\n","print(f\"ì‹¤ì œê°’ ì˜ˆì‹œ: {y_test[:5]}\")\n","print(f\"ì˜ˆì¸¡ê°’ ì˜ˆì‹œ: {y_pred[:5]}\")\n","\n","# 1. MSE (Mean Squared Error) - í‰ê·  ì œê³± ì˜¤ì°¨\n","mse = mean_squared_error(y_test, y_pred)\n","print(f\"\\n1ï¸âƒ£ MSE: {mse:.2f}\")\n","print(\"   ì˜ë¯¸: ì˜¤ì°¨ì˜ ì œê³± í‰ê·  (ì´ìƒì¹˜ì— ë¯¼ê°)\")\n","print(\"   í•´ì„: ê°’ì´ ì‘ì„ìˆ˜ë¡ ì¢‹ìŒ\")\n","\n","# 2. MAE (Mean Absolute Error) - í‰ê·  ì ˆëŒ€ ì˜¤ì°¨\n","mae = mean_absolute_error(y_test, y_pred)\n","print(f\"\\n2ï¸âƒ£ MAE: {mae:.2f}\")\n","print(\"   ì˜ë¯¸: ì˜¤ì°¨ì˜ ì ˆëŒ“ê°’ í‰ê·  (ì´ìƒì¹˜ì— ê°•ê±´)\")\n","print(\"   í•´ì„: ê°’ì´ ì‘ì„ìˆ˜ë¡ ì¢‹ìŒ, ì§ê´€ì  ì´í•´ ì‰¬ì›€\")\n","\n","# 3. RÂ² Score - ê²°ì •ê³„ìˆ˜\n","r2 = r2_score(y_test, y_pred)\n","print(f\"\\n3ï¸âƒ£ RÂ² Score: {r2:.3f}\")\n","print(\"   ì˜ë¯¸: ëª¨ë¸ì´ ì„¤ëª…í•˜ëŠ” ë¶„ì‚° ë¹„ìœ¨\")\n","print(\"   í•´ì„: 0~1, 1ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ì¢‹ìŒ\")\n","\n","# ê°„ë‹¨í•œ ì„±ëŠ¥ í‰ê°€\n","if r2 > 0.8:\n","    print(\"\\nâœ… ìš°ìˆ˜í•œ ëª¨ë¸ ì„±ëŠ¥\")\n","elif r2 > 0.6:\n","    print(\"\\nâœ… ì–‘í˜¸í•œ ëª¨ë¸ ì„±ëŠ¥\")\n","else:\n","    print(\"\\nâš ï¸ ëª¨ë¸ ê°œì„  í•„ìš”\")"],"id":"djyDIbjmTDag"},{"cell_type":"markdown","source":["### ì˜ˆì œ : ë¶„ë¥˜ëª¨ë¸ í‰ê°€ì§€í‘œ"],"metadata":{"id":"pvuBcp_ecq63"},"id":"pvuBcp_ecq63"},{"cell_type":"markdown","source":["|ì§€í‘œ | ê³µì‹ | ì–¸ì œ ì¤‘ìš”í•œê°€ | í•´ì„|\n","|---|---|---|--|\n","|Accuracy | (TP+TN)/(TP+TN+FP+FN) | í´ë˜ìŠ¤ê°€ ê· í˜•ì¡íŒ ê²½ìš° | 1ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ì¢‹ìŒ|\n","|Precision | TP/(TP+FP) | ì˜¤íƒ(False Positive)ì´ ì¹˜ëª…ì ì¼ ë•Œ | 1ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ì¢‹ìŒ|\n","|Recall | TP/(TP+FN) | ë†“ì¹¨(False Negative)ì´ ì¹˜ëª…ì ì¼ ë•Œ | 1ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ì¢‹ìŒ|\n","|F1-Score | 2Ã—(PrecisionÃ—Recall)/(Precision+Recall) | ê· í˜•ì¡íŒ í‰ê°€ê°€ í•„ìš”í•  ë•Œ | 1ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ì¢‹ìŒ|"],"metadata":{"id":"Vyqwhp8GdnyR"},"id":"Vyqwhp8GdnyR"},{"cell_type":"code","source":["import numpy as np\n","from sklearn.datasets import make_classification\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n","\n","# ìƒ˜í”Œ ë¶„ë¥˜ ë°ì´í„° ìƒì„±\n","X, y = make_classification(n_samples=1000, n_features=10, n_classes=2, random_state=42)\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# ëª¨ë¸ í›ˆë ¨ ë° ì˜ˆì¸¡\n","model = LogisticRegression(random_state=42)\n","model.fit(X_train, y_train)\n","y_pred = model.predict(X_test)\n","\n","print(\"=== ë¶„ë¥˜ ëª¨ë¸ í‰ê°€ì§€í‘œ 4ê°€ì§€ ===\")\n","print(f\"ì‹¤ì œê°’ ì˜ˆì‹œ: {y_test[:10]}\")\n","print(f\"ì˜ˆì¸¡ê°’ ì˜ˆì‹œ: {y_pred[:10]}\")\n","\n","# 1. Accuracy (ì •í™•ë„)\n","accuracy = accuracy_score(y_test, y_pred)\n","print(f\"\\n1ï¸âƒ£ Accuracy: {accuracy:.3f}\")\n","print(\"   ì˜ë¯¸: ì „ì²´ ì¤‘ ë§ê²Œ ì˜ˆì¸¡í•œ ë¹„ìœ¨\")\n","print(\"   í•´ì„: 1ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ì¢‹ìŒ\")\n","\n","# 2. Precision (ì •ë°€ë„)\n","precision = precision_score(y_test, y_pred)\n","print(f\"\\n2ï¸âƒ£ Precision: {precision:.3f}\")\n","print(\"   ì˜ë¯¸: ì–‘ì„±ìœ¼ë¡œ ì˜ˆì¸¡í•œ ê²ƒ ì¤‘ ì‹¤ì œ ì–‘ì„±ì¸ ë¹„ìœ¨\")\n","print(\"   í•´ì„: ê±°ì§“ ì–‘ì„±(ì˜¤íƒ)ì„ ì¤„ì´ê³  ì‹¶ì„ ë•Œ ì¤‘ìš”\")\n","\n","# 3. Recall (ì¬í˜„ìœ¨)\n","recall = recall_score(y_test, y_pred)\n","print(f\"\\n3ï¸âƒ£ Recall: {recall:.3f}\")\n","print(\"   ì˜ë¯¸: ì‹¤ì œ ì–‘ì„± ì¤‘ ì˜¬ë°”ë¥´ê²Œ ì˜ˆì¸¡í•œ ë¹„ìœ¨\")\n","print(\"   í•´ì„: ê±°ì§“ ìŒì„±(ë†“ì¹¨)ì„ ì¤„ì´ê³  ì‹¶ì„ ë•Œ ì¤‘ìš”\")\n","\n","# 4. F1-Score (F1 ì ìˆ˜)\n","f1 = f1_score(y_test, y_pred)\n","print(f\"\\n4ï¸âƒ£ F1-Score: {f1:.3f}\")\n","print(\"   ì˜ë¯¸: Precisionê³¼ Recallì˜ ì¡°í™”í‰ê· \")\n","print(\"   í•´ì„: ê· í˜•ì¡íŒ ì„±ëŠ¥ í‰ê°€\")\n","\n","# í˜¼ë™í–‰ë ¬ (Confusion Matrix)\n","cm = confusion_matrix(y_test, y_pred)\n","print(f\"\\nğŸ“Š í˜¼ë™í–‰ë ¬:\")\n","print(f\"   ì‹¤ì œ\\\\ì˜ˆì¸¡   0    1\")\n","print(f\"        0    {cm[0,0]:3}  {cm[0,1]:3}\")\n","print(f\"        1    {cm[1,0]:3}  {cm[1,1]:3}\")\n","\n","# ê°„ë‹¨í•œ ì„±ëŠ¥ í‰ê°€\n","print(f\"\\n=== ì„±ëŠ¥ í‰ê°€ ===\")\n","if accuracy > 0.9:\n","    print(\"âœ… ë§¤ìš° ìš°ìˆ˜í•œ ì„±ëŠ¥\")\n","elif accuracy > 0.8:\n","    print(\"âœ… ìš°ìˆ˜í•œ ì„±ëŠ¥\")\n","elif accuracy > 0.7:\n","    print(\"âœ… ì–‘í˜¸í•œ ì„±ëŠ¥\")\n","else:\n","    print(\"âš ï¸ ì„±ëŠ¥ ê°œì„  í•„ìš”\")\n","\n","# ì •ë°€ë„ vs ì¬í˜„ìœ¨ íŠ¸ë ˆì´ë“œì˜¤í”„\n","if precision > recall:\n","    print(\"ğŸ“ˆ ì •ë°€ë„ê°€ ë†’ìŒ: ì˜¤íƒì´ ì ìŒ\")\n","else:\n","    print(\"ğŸ“ˆ ì¬í˜„ìœ¨ì´ ë†’ìŒ: ë†“ì¹˜ëŠ” ê²ƒì´ ì ìŒ\")"],"metadata":{"id":"pwmeoI5Ucwta"},"id":"pwmeoI5Ucwta","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **[Tip] ëª¨ë¸ì˜ íŒŒë¼ë¯¸í„° í™•ì¸í•˜ëŠ” ë°©ë²•**"],"metadata":{"id":"Pc3lV6uLEFr4"},"id":"Pc3lV6uLEFr4"},{"cell_type":"code","source":["# ëª¨ë¸ì˜ íŒŒë¼ë¯¸í„° í™•ì¸í•˜ëŠ” ë°©ë²•\n","print(model.get_params().keys())  # íŒŒë¼ë¯¸í„° ì´ë¦„ë“¤\n","print(model.get_params())         # íŒŒë¼ë¯¸í„° ì „ì²´ì™€ ê°’"],"metadata":{"id":"4bOVE-2nDtdV"},"id":"4bOVE-2nDtdV","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","\n","---\n","\n"],"metadata":{"id":"69ZO4NC1l-Js"},"id":"69ZO4NC1l-Js"}],"metadata":{"colab":{"provenance":[],"toc_visible":true},"language_info":{"name":"python"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"nbformat":4,"nbformat_minor":5}